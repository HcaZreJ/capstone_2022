{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiKey = 'e938341216df4163be5f15cb92d413e6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "# import json\n",
    "\n",
    "################################## Web-crawling News data ##################################\n",
    "# 1. Set a few News websites to crawl from\n",
    "# 2. Use Newsapi to crawl all of their news article links in the past 3 years\n",
    "# 3. Crawl all of the news article's content from these links and store as data\n",
    "# 4. Format all of the news article content to make them vectorizable by a word2vec model\n",
    "# 5. Load the pre-trained Google-News Word2Vec model & turn all news article content into\n",
    "#       an average of all of the vector of its words\n",
    "# 6. Create a similarity matrix that stores the similarity between every pair of news articles\n",
    "# Everything above just need to be packged into a function + make a few changes to make it\n",
    "# ready for integration into a web app.\n",
    "# 7. Test case - Simulating some random user myself\n",
    "\n",
    "##############################     1     ##############################\n",
    "# Set the sources that we are going to crawl from\n",
    "# Since this is a draft we won't worry about #sourcequality yet.\n",
    "# The sources are decided from this list: https://www.top10.com/news-websites\n",
    "# There isn't New York Times & NPR in Newsapi, so it won't be included either for now\n",
    "# Google News is a search engine that shows news from other news websites, which makes crawling\n",
    "# too hard, so won't be included either\n",
    "sources = 'cnn,reuters,the-wall-street-journal,bbc-news,fox-news,nbc-news,the-washington-post'\n",
    "# Note: To see all of the sources offered by Newsapi, visit: https://newsapi.org/v2/sources?apiKey=e938341216df4163be5f15cb92d413e6\n",
    "\n",
    "# Use a dictionary of dictionaries to set the tags & attributes for which news\n",
    "# content is stored in each of the above news sources, will be used later in crawling\n",
    "contentLocation = {\n",
    "    'cnn': {\n",
    "        'tag': 'p',\n",
    "        'class': 'paragraph inline-placeholder'\n",
    "    },\n",
    "    'reuters': {\n",
    "        'tag': 'p',\n",
    "        'class': 'text__text__1FZLe text__dark-grey__3Ml43 text__regular__2N1Xr text__large__nEccO body__full_width__ekUdw body__large_body__FV5_X article-body__element__2p5pI'\n",
    "    },\n",
    "    'the-wall-street-journal': {\n",
    "        'tag': 'p',\n",
    "        'class': 'css-xbvutc-Paragraph e3t0jlg0'\n",
    "    },\n",
    "    'bbc-news': {\n",
    "        'tag': 'p',\n",
    "        'class': 'ssrcss-1q0x1qg-Paragraph eq5iqo00'\n",
    "    },\n",
    "    'fox-news': {\n",
    "        'tag': 'p' # Fox news' article content has no attribute but just a tag <p>\n",
    "    },\n",
    "    'nbc-news': {\n",
    "        'tag': 'p',\n",
    "        'class': '' # Content is stored under the class attribute with no value on NBC news\n",
    "    },\n",
    "    'the-washington-post': {\n",
    "        'tag': 'p',\n",
    "        'class': 'wpds-c-cYdRxM wpds-c-cYdRxM-iPJLV-css font-copy'\n",
    "    }\n",
    "}\n",
    "\n",
    "##############################     2     ##############################\n",
    "# use headers to hide our API key\n",
    "headers = {'Authorization': 'e938341216df4163be5f15cb92d413e6'}\n",
    "\n",
    "# Set the API endpoint to crawl data from\n",
    "everything = \"https://newsapi.org/v2/everything?\"\n",
    "top_headlines = \"https://newsapi.org/v2/top-headlines?\"\n",
    "\n",
    "# Define keyword for how sources will be sorted\n",
    "sorby = \"popularity\"\n",
    "\n",
    "# Store keywords into a dictionary for use in crawling\n",
    "params = {'apiKey': apiKey,\n",
    "          'sources': sources,\n",
    "          'sortBy': sorby,\n",
    "          'language': 'en',\n",
    "          'page': 1}\n",
    "\n",
    "# Set html requests and get a response object, this is first run to establish\n",
    "# the total number of articles that we need to crawl\n",
    "response = requests.get(url = everything, headers = headers, params = params)\n",
    "\n",
    "# Turn response into a json object, and get the 'totalResults' field\n",
    "output = response.json()\n",
    "totalResults = output['totalResults']\n",
    "# Since each crawl gives a maximum of 100 articles, we would need to crawl\n",
    "# floor division by 100 + 1 times in total\n",
    "totalCrawlsNeeded = totalResults//100 + 1\n",
    "\n",
    "# However, due to using an upaid plan, I am actually only allowed 100 requests\n",
    "# per day. Thus this will be instead set to 100\n",
    "totalCrawlsNeeded = 100\n",
    "\n",
    "# Crawl all of the article urls and store them into our dataframe\n",
    "for i in range(1, totalCrawlsNeeded+1):\n",
    "    # Set the page number crawled in this run\n",
    "    params['page'] = i\n",
    "\n",
    "    # Send html requests and get response\n",
    "    response = requests.get(url = everything, headers = headers, params = params)\n",
    "\n",
    "    # Turn response into a json object\n",
    "    output = response.json()\n",
    "\n",
    "    ## Turn the json result into a pandas.dataframe object\n",
    "\n",
    "    # Our unpaid version limits the number of urls we can get, thus we need to wrap\n",
    "    # the next line in a try, except block\n",
    "    try:\n",
    "        # Variable to hold the list of article information\n",
    "        articles = output['articles']\n",
    "    except KeyError:\n",
    "        break\n",
    "\n",
    "    # Create dataframe from the list of dictionaries, if not present, else just concatenate\n",
    "    try:\n",
    "        df\n",
    "        temp = pd.DataFrame(articles)\n",
    "        df = pd.concat([df, temp], ignore_index = True)\n",
    "    except NameError:\n",
    "        df = pd.DataFrame(articles)        \n",
    "\n",
    "    # Sleep for 2 seconds to avoid overloading the API\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################     3     ##############################\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "\n",
    "def get_news_content(url, tag, class_=None):\n",
    "    '''\n",
    "    Crawls a new's content, given the url, tag, and class.\n",
    "\n",
    "    Returns True, news_content if crawling was successful, else\n",
    "    returns False, [url, error_type, error_info] if error was encountered.\n",
    "    '''\n",
    "    # Since our code might produce errors in the process for various\n",
    "    # reasons, calling it in a try-except block will make the code run better\n",
    "    try:\n",
    "        # Send requests to the url & obtain response object\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Use BeautifulSoup to parse the html response & finding data\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the news content using the given tag & attribute\n",
    "        if class_:\n",
    "            content = soup.find_all(tag, class_ = class_)\n",
    "        else:\n",
    "            content = soup.find_all(tag)\n",
    "\n",
    "        # Content is a list of all the html elements found, we need to \n",
    "        # further concatenate them together into a string and strip it\n",
    "        news_content = ''\n",
    "        for tag_found in content:\n",
    "            news_content += tag_found.text + ' '\n",
    "\n",
    "        # Return True & content\n",
    "        return True, news_content\n",
    "\n",
    "    # this describes what to do if an exception is thrown \n",
    "    except Exception:\n",
    "        \n",
    "        # get the exception information\n",
    "        error_type, error_obj, error_info = sys.exc_info()\n",
    "        \n",
    "        # Return False & error info\n",
    "        return False, [url, error_type, error_obj, error_info]\n",
    "\n",
    "# Create a list to hold all of the failure info\n",
    "failure_info = []\n",
    "\n",
    "# Crawl news article data from all of the urls in the df\n",
    "for index, row in df.iterrows():\n",
    "    # Get the url\n",
    "    url = row['url']\n",
    "\n",
    "    # Get the id of the news website, then obtain tag & class info\n",
    "    # using our predefined dictionary\n",
    "    id = row['source']['id']\n",
    "    tag = contentLocation[id]['tag']\n",
    "    # Fox news is the only news website where its article content\n",
    "    # doesn't have any html attribute but just a tag <p>\n",
    "    if contentLocation[id] != 'fox-news':\n",
    "        class_ = contentLocation[id]['class']\n",
    "\n",
    "    # Crawl news content given the inputs\n",
    "    if class_:\n",
    "        successful, content = get_news_content(url, tag, class_)\n",
    "    else:\n",
    "        successful, content = get_news_content(url, tag)\n",
    "\n",
    "    # If successful, then replace the 'content' section of our df with the content\n",
    "    # Which is the last column, thus can be accessed by df.iloc[index, -1]\n",
    "    if successful:\n",
    "        df.iloc[index, -1] = content\n",
    "    # If failed then we gather the failure's info\n",
    "    else:\n",
    "        failure_info.append(content)\n",
    "\n",
    "    # Sleep for 2 seconds to avoid overloading\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(failure_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\#cs110-PythonProgramming: Explicitly shows that the PythonProgram passes verification tests.\n",
    "\n",
    "The placeholder list failure_info is completely empty shows that the above function of get_news_content() worked perfectly without any errors. There are certain runs in which the news_content variable returned from the function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################     4     ##############################\n",
    "# First we need to do some formatting. Right now the 'content' column\n",
    "# of our df is really messy. Let's define a function to do that\n",
    "def format_string(text):\n",
    "    '''\n",
    "    Formats a string of text to make it more standardized. Includes\n",
    "    the following operations in the exact order:\n",
    "    1. Removes contractions, e.g. I'll -> I will\n",
    "    2. Removes punctuations, e.g. That is it. -> That is it\n",
    "    3. Removes numbers, e.g. 300 turtles -> turtles\n",
    "    4. Removes extra space, e.g. you  are right -> you are right\n",
    "    5. Makes words lowercase, e.g. Terminal -> terminal\n",
    "    6. Removes stop words (i.e. words that don't add value to our analysis),\n",
    "        e.g. the library -> library\n",
    "    '''\n",
    "    # 1. Removes contractions\n",
    "    from contractions import fix\n",
    "    text = fix(text)\n",
    "\n",
    "    # 2. Removes punctuation\n",
    "    from string import punctuation\n",
    "    translator = str.maketrans(punctuation, ' '*len(punctuation)) # map punctuation to space\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # 3. Removes numbers\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    # 4. Removes extra space\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # 5. Makes lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 6. Removes stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    text = \" \".join([word for word in text.split() if word not in (stop)])\n",
    "\n",
    "    # Return the result\n",
    "    return text\n",
    "\n",
    "# Next we are going to format all of the news article content\n",
    "# Define a list to hold all of the formatted strings\n",
    "formatted_contents = []\n",
    "for index, row in df.iterrows():\n",
    "    # Format the text\n",
    "    formatted_string = format_string(row['content'])\n",
    "\n",
    "    # Add the formatted text to list\n",
    "    formatted_contents.append(formatted_string)\n",
    "\n",
    "# Insert this list as a new column into our df\n",
    "df['Formatted content'] = formatted_contents\n",
    "\n",
    "# If formatted content is empty, then something went wrong in the previous\n",
    "# process (likely with web crawling). To avoid it interrupting subsequent\n",
    "# code, we are going to delete the row from the df\n",
    "delete_indexes = []\n",
    "for index, row in df.iterrows():\n",
    "    if not row['Formatted content']:\n",
    "        delete_indexes.append(index)\n",
    "\n",
    "df = df.drop(index = delete_indexes)\n",
    "# Reset indexes\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################     5     ##############################\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-tained google-news model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Define a function, that takes the model & a string of text as input\n",
    "# and outputs the averaged vector over the text.\n",
    "def text_to_average_vector(model, text):\n",
    "    '''\n",
    "    Using the gensim model, converts each word in the text string into\n",
    "    a vector, averages over all these vectors, and returns the average.\n",
    "    '''\n",
    "    # Split the text into a list of words\n",
    "    words = text.split()\n",
    "\n",
    "    # Create an empty numpy array, with ncol = number of words, and\n",
    "    # nrow = output dimensions of the model\n",
    "    all_vectors = np.zeros((model.vector_size, len(words)))\n",
    "\n",
    "    # If our model is large, then it would be reasonable for us to assume that \n",
    "    # any word that cannot be found in the model is not a word, e.g. â or ©\n",
    "    # If such words are encountered, it also means that we need to delete a\n",
    "    # column from our vector of all the words, to avoid disrupting the subsequent\n",
    "    # average, thus define a list to hold the column indexes that we are going to\n",
    "    # delete later:\n",
    "    col_to_delete = []\n",
    "\n",
    "    # Looping over all words, turn them into vectors and insert into np array\n",
    "    for i in range(len(words)):\n",
    "        try:\n",
    "            vector = model.get_vector(words[i])\n",
    "        # If the word cannot be found in the model, we will need to delete this\n",
    "        # column from our array of vectors\n",
    "        except KeyError:\n",
    "            col_to_delete.append(i)\n",
    "            continue\n",
    "\n",
    "        # Insert this vector at the right position\n",
    "        # a[:, 0] means select all rows from column 0\n",
    "        all_vectors[:, i] = vector\n",
    "\n",
    "    # Delete the columns in which the word cannot be found in the model\n",
    "    final_vec = np.delete(all_vectors, col_to_delete, axis = 1)\n",
    "\n",
    "    # Average over the columns, and return the averaged vector\n",
    "    averaged_vec = np.mean(final_vec, axis = 1)\n",
    "    return averaged_vec\n",
    "\n",
    "# For all of our news articles, obtain an averaged vector of its news content,\n",
    "# and add as a new column to our df\n",
    "df_vectors = []\n",
    "for index, row in df.iterrows():\n",
    "    news_vector = text_to_average_vector(model, row['Formatted content'])\n",
    "    df_vectors.append(news_vector)\n",
    "\n",
    "df['Vector'] = df_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################     6     ##############################\n",
    "# I've tried the n_similarity function given by the model, but it did\n",
    "# not work so well. I think it is because of the enormous amount of words\n",
    "# that made the model think every pair of articles is similar. The minimum\n",
    "# of all of them is 0.97 (which does not make enough sense), and it takes\n",
    "# a long time to run as well. Thus instead I created an average vector on my\n",
    "# own, and computed their cosine similarity. It is still not very good, but\n",
    "# it's better than all news articles being similar (good enough for a 1st draft).\n",
    "\n",
    "similarity_matrix = np.zeros((len(df), len(df)))\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Looping over all the rows in df\n",
    "for index1, row1 in df.iterrows():\n",
    "    # Compare row's vector against every other row's vector\n",
    "    for index2, row2 in df.iterrows():\n",
    "        # Reshape the vectors for use in sklearn's function\n",
    "        vec1 = row1['Vector'].reshape(1, -1)\n",
    "        vec2 = row2['Vector'].reshape(1, -1)\n",
    "\n",
    "        # Compute score using cosine similarity\n",
    "        score = cosine_similarity(vec1, vec2)\n",
    "\n",
    "        # Add the score to our matrix\n",
    "        similarity_matrix[index1, index2] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.89798543, 0.77919876, ..., 0.80570019, 0.79856328,\n",
       "        0.76625432],\n",
       "       [0.89798543, 1.        , 0.76081528, ..., 0.77043512, 0.76672343,\n",
       "        0.73776677],\n",
       "       [0.77919876, 0.76081528, 1.        , ..., 0.67246047, 0.78875015,\n",
       "        0.80760098],\n",
       "       ...,\n",
       "       [0.80570019, 0.77043512, 0.67246047, ..., 1.        , 0.83692341,\n",
       "        0.76949084],\n",
       "       [0.79856328, 0.76672343, 0.78875015, ..., 0.83692341, 1.        ,\n",
       "        0.79670984],\n",
       "       [0.76625432, 0.73776677, 0.80760098, ..., 0.76949084, 0.79670984,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In a deployment environment, this matrix will be stored in the server,\n",
    "# and updated every once in a while\n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n      In my most vulnerable moments, I have pictured having to tell my young child that his moms’ marriage is no longer recognized. I imagined choking back tears and reminding him that it doesn’t mean he is any less loved or protected. I envisioned telling him that our family is just as important as any other family – including ones with one mom and one dad. \\n   \\n      Thankfully, Congress just ensured I will not have to have that conversation with my child anytime soon. \\n   \\n      On Tuesday, the Senate voted to pass the Respect for All Marriage Act, a law that received a strong showing of bipartisan support in the House some months ago. It now goes to the House for a final vote, where it is expected to pass before the end of the year.\\n   \\nThe Respect for All Marriage Act repeals the long-outdated Defense of Marriage Act, which defined marriage as between one man and one woman and allowed states to deny recognition of same-sex marriages that originated in states where they were legally recognized. The new bill also codifies that same-sex marriage is recognized federally and that same-sex spouses get the same federal benefits that all married couples receive.\\n   \\n      In many ways, the bill is the eventuality that many advocates wanted to pursue more than a decade ago when there was momentum at the state level and marriage equality was passing in state legislatures. The hope was that a federal bill would help enshrine marriage recognition across all states – rather than rely on the Supreme Court, which could reverse a decision in favor of same-sex marriage someday, just as it did when it overturned Roe v. Wade in June.\\n   \\n      That said, the law is not the be-all, end-all showstopper that allows us to recycle the protest signs and go home.\\n   \\n      The bill contains carveouts by a bipartisan Senate cohort intended to appease Republicans and achieve the necessary 60 votes for passage. It gives religious institutions and organizations the right to define marriage as between one man and one woman and to refuse service to same-sex couples and families. \\n   \\n      It further upholds the Religious Freedom Restoration Act, which was originally intended to protect religious minorities from discrimination by the state but has since been twisted by religious zealots and evangelists to refuse service to LGBTQ+ people on the grounds that it violates their deeply held religious belief. \\n   \\n      It would allow religiously-affiliated adoption agencies the right to refuse to place foster children in an LGBTQ+ household, or allow a religious school the right to refuse to hire an LGBTQ+ teacher. \\n   \\n      The law also leaves in place state-level bans on marriage equality that, should the Supreme Court overturn same-sex marriage, would return the country to a patchwork legal landscape similar to the one we saw before the court decision requiring states to recognize same-sex marriage. Holy confusing taxes, medical care and so many other things.\\n   \\n      And it’s not game over for LGBTQ+ rights when the bill gets signed into law. Marriage is but one arena in civic life where LGBTQ+ people continue to face inequities and discrimination. We still don’t have blanket protections in housing or public accommodations, which the Equality Act would address, but which has not yet come to a floor vote in the Senate. And we continue to be exploited as wedge issues for right-wing platforms, as we’ve seen play out with hundreds of bills at the state level across 2022 legislatures.\\n   \\n      Despite these lingering challenges, Tuesday’s vote is a significant win – and it rightly reflects the will of the majority. 71% of Americans support marriage equality, per a record-high May 2022 Gallup poll. \\n   \\n      If the Supreme Court were to pull the rug out from under LGBTQ+ Americans, it would mean potentially tearing apart families with children and destroying fiscal units that strengthen our communities and economies. One only needs to look to Justice Clarence Thomas’ majority opinion in the Dobbs decision this past June, where he called for the overturning of marriage equality, to validate the fear.\\n   \\n      “No American should ever, ever be discriminated against because of whom they love, and passing this bill would secure these much-needed safeguards into Federal law,” Sen. Chuck Schumer said when vowing to bring the bill up for a vote. \\n   \\n      “I want to make clear that passing this bill is not a theoretical exercise… The rights and dignity of millions of Americans depend on it,” he explained. Indeed, my family depends on it.\\n   \\n      I can hear the collective sigh of relief from millions of LGBTQ+ Americans and our loved ones.\\n   '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################     7     ##############################\n",
    "# (Intended) Upon login, 10 random articles is shown to the user to\n",
    "# learn their preference.\n",
    "# Choose 10 random articles, show them to the user, obtain their \n",
    "# satisfaction score on these articles on a scale of 1 to 10, with\n",
    "# 10 the most satisfied and 1 being the least satisfied, then compute\n",
    "# the user's preference vector by averaging over these vectors using\n",
    "# their rating as the weights.\n",
    "\n",
    "# Set numpy random seed to ensure replicability\n",
    "np.random.seed(125)\n",
    "ten_random_articles = np.random.choice(df.index, size=10, replace=False)\n",
    "\n",
    "# Holder variable to record vector and score\n",
    "user_scores = []\n",
    "\n",
    "# Looping over the 10 articles\n",
    "for index in ten_random_articles:\n",
    "    # Show the article to the user and inquire a score from the user\n",
    "    score = int(input(\"Please give a satisfaction score in the range of 1 to 10 on the recommended article: \\n\\n\" + df['content'][index]))\n",
    "\n",
    "    # Turn it into a dictionary and add to holder list\n",
    "    temp = {'vector': df['Vector'][index],\n",
    "            'score' : score}\n",
    "    user_scores.append(temp)\n",
    "\n",
    "# Obtain an average of the user's vector weighted by score, to generate the initial\n",
    "# learned preference of the user\n",
    "sum_vector = np.zeros((model.vector_size, ))\n",
    "for i in range(len(user_scores)):\n",
    "    weighted_score = user_scores[i]['score'] / 10\n",
    "    \n",
    "    sum_vector += weighted_score * user_scores[i]['vector']\n",
    "\n",
    "# Average over all vectors to get the initial preference\n",
    "user_preference = sum_vector / len(user_scores)\n",
    "user_preference = user_preference.reshape(1, -1)\n",
    "\n",
    "# Find the article that is most similar to the learned preference of the user,\n",
    "# Excluding the articles that was used for training\n",
    "df_user = df.drop(index=ten_random_articles)\n",
    "similarities = []\n",
    "for index, row in df_user.iterrows():\n",
    "    vector_article = row['Vector'].reshape(1, -1)\n",
    "    sim = cosine_similarity(user_preference, vector_article)\n",
    "    similarities.append(sim)\n",
    "\n",
    "index = similarities.index(max(similarities))\n",
    "\n",
    "df['content'][index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analysis\n",
    "\n",
    "The above results are reasonable and expected, since I gave two ratings of 9 to two Ukrainian war news articles, and two 8s and one 7 to three other political articles (if you want to check out the articles out themselves, you can run all of the above code cells, it should output the same data), and the most similar piece of text that the model recommends is also about the Ukrainian war. It makes perfect sense for the recommender to be recommending news articles about the Ukrainian war, after it sees that I am interested in the Ukranian war and politics. This is defintely not the ideal version of the recommender that we want, since we want it to be able to extrapolate our preference and recommend thing that we do not know we are interested in.\n",
    "\n",
    "I have thought about a few interesting applications that might strength this current Word2Vec model, and I will be testing one of them out in the next iteration of the product. They are:\n",
    "\n",
    "1. Instead of recommending articles that has the closest similarity to the learned user's preference, we always attempt to recommend articles that are a given distance away from the user's preference. In other words, if we were to think of the endpoint of the vector of user's preference as a point in a high-dimensional word-embedding space, we always attempt to recommend articles that is on the surface of a hyper-sphere that has its center as the preference of the user. With every news article that we recommend to the user, instead of asking for the user to score their satisfaction, we ask the user whether this is too convergent to their ideas (by which we move the center of the hyper-sphere away from this point), or too divergent to their ideas (by which we move the center of the hyper-sphere closer to this point); and whether they would like more convergent new (by which we decrease the radius of the hyper-sphere), or they would like more divergent news (by which we then increase the radius of the hyper-sphere).\n",
    "\n",
    "2. The limitation of the above method is that it allows users to create their own \"media bubble\". Stubborn people only gets more stubborn. A potential improvement to the model is if we do not allow the user to control that. In addition, we include some randomness into the model prediction, by adding a random vector of a set magnitude on top of the learned user's preference every time we are generating a news recommendation, that is to say, instead of recommending news articles that are the most similar to the preference of the user, we recommend news articles that are always a set semantic distance away from the preference of the user (in a random direction). Obviously, the limitation of this method is that the setting of the magnitude of the random vector becomes very influential in the recommendation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'France has suspended a plan to take in 3,500 refugees currently in Italy after Rome refused to let a migrant rescue ship disembark on its shores. Tensions between the two neighbours have escalated since Italy\\'s new government barred the Ocean Viking ship from docking with 230 migrants. France has denounced the \"unacceptable behaviour\", but Italy insists it has been taking in its share of migrants. The boat will be allowed to dock on Friday in the French port of Toulon. The charity that runs the vessel, SOS MÃ©diterranÃ©e, said it was both relieved by the French decision and angry that for three weeks those on board had been let down by Europe\\'s \"dramatic failure\". French Interior Minister GÃ©rald Darmanin warned of \"extremely severe consequences for our bilateral relations\" with Italy. For a start, France would not take in 3,500 refugees it had earlier agreed with Italy to accept in protest at Rome\\'s actions. He also said France would take measures to strengthen controls on the border with Italy. His Italian counterpart, Matteo Piantedosi, later hit back - saying his neighbour\\'s actions were \"totally incomprehensible\" as Italy had taken in some 90,000 migrants this year alone. He questioned \"why Italy should willingly accept something that others are not willing to accept\". Italy\\'s right-wing government came to power last month under Giorgia Meloni, who before she became prime minister vowed to blockade migrant boats trying to leave North Africa for Italy. The Ocean Viking was one of four charity ships that sailed for Italy with a total of more than 500 migrants rescued in the Mediterranean. Those on board the other three ships were eventually allowed to disembark at Italian ports, but only after initial refusals. At one point three of those on board the Geo Barents rescue boat leapt into the water to reach land.  Ocean Viking was denied entry into port and SOS MÃ©diterranÃ©e said its 43 requests to Italian authorities met with no response.   It sailed on towards Corsica and four people on board were airlifted to hospital on Thursday morning. SOS MÃ©diterranÃ©e, cited by AFP, said one of the migrants on the ship was in an unstable condition and had not reacted to treatment since late October. The French interior minister then announced the others would be allowed to leave the ship at the naval base in Toulon on an \"exceptional\" basis.  A third of passengers would be \"relocated\" to France, he said, while another third would be sent to Germany and the others shared out between EU member states.  He went on to criticise Rome\\'s \"reprehensible\" attitude and stressed the migrants had been picked up in Italy\\'s search-and-rescue zone in the Mediterranean. The European Commission said on Wednesday that the ship should be able to immediately disembark at the nearest place of safety. In a statement, it did not mention Italy or France by name, but said there was a \"clear and unequivocal\" legal obligation to rescue distressed persons at sea. French far-right leader Marine Le Pen reacted angrily to France\\'s decision to allow the Ocean Viking to disembark in Toulon, accusing President Emmanuel Macron of a dramatic show of lenience towards \"massive and anarchic immigration\". This video can not be played Watch: Migrant rescue boat still waiting for permission to dock in Italy Â© 2022 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking. '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random as rd\n",
    "# Prototype of how user preference learning will be like (next step)\n",
    "# Here's how I imagined the user preference learning process to be:\n",
    "# 1. User logins onto their account and their preference will be learned\n",
    "#       by supplying 10 articles for them to rate on a scale of 1 to 10,\n",
    "#       with respect to how satisfied they are with the article with 10\n",
    "#       being the most satisfied and 1 being not satisfied at all.\n",
    "# 2. After the rating of each article, we learn a little bit more about\n",
    "#       the user. We aim to give user the most semantically different\n",
    "#       article to rate (by maximizing the difference in similarity\n",
    "#       score compared to their previous ratings), to get more info of\n",
    "#       the user.\n",
    "# 3. After the user rates all 10 articles, we will get an average of these\n",
    "#       vectors, weighted by the ratings they gave to each of them. This\n",
    "#       will be the initial learned preference of the user.\n",
    "# 4. Whenever the user clicks on an article recommended to them (which pops\n",
    "#       up in a separate window), the website will show a window which asks\n",
    "#       them to rate the article that they just read on a scale of 1 to 10.\n",
    "#       We use this to learn a little bit further about the user.\n",
    "\n",
    "\n",
    "# Taking myself as the test case, I am going to simulate being a user here\n",
    "# Create a holder list that will hold all of the vectors that the user\n",
    "# has rated, the index of the article, and their rating of the articles\n",
    "zichen = []\n",
    "# Set a seed to ensure replicability\n",
    "rd.seed(12454)\n",
    "# Determine a random first article to show\n",
    "first_article_index = rd.choice(df.index)\n",
    "\n",
    "# Hidden here to avoid excessive amount of output, but basically it was\n",
    "# about shooting crimes in Virginia which I honestly don't care for,\n",
    "# so I am going to give it a score of 1\n",
    "#df['content'][first_article_index]\n",
    "rating1 = {'vector': df['Vector'][first_article_index],\n",
    "           'index' : first_article_index,\n",
    "           'score' : 1}\n",
    "zichen.append(rating1)\n",
    "\n",
    "# Next we are going to find the most different article from the first one\n",
    "# to give to the user for rating, so lets find the least similar article\n",
    "# from our similarity matrix compared to the first article shown\n",
    "first_sim = similarity_matrix[first_article_index]\n",
    "second_article_indexes = np.where(first_sim == first_sim.min())\n",
    "\n",
    "# We found two, 63 and 147, so let's just randomly choose one\n",
    "if rd.random() < 0.5:\n",
    "    second_article_index = 63\n",
    "else:\n",
    "    second_article_index = 147\n",
    "\n",
    "# The random run gave 63, now show the second article\n",
    "# Hidden to avoid excessive output\n",
    "df['content'][second_article_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sr/pgt_1qtd02s8mtf331by6sm80000gn/T/ipykernel_2977/1510789129.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "python.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de264d6f37cb5440ad063971cae3b1d83d14f2649fd07ea20b1c93af5fbd4acc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
