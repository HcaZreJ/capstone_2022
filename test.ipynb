{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiKey = 'e938341216df4163be5f15cb92d413e6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "# import json\n",
    "\n",
    "################################## Web-crawling News data ##################################\n",
    "# 1. Set a few News websites to crawl from\n",
    "# 2. Use Newsapi to crawl all of their news article links in the past 3 years\n",
    "# 3. Crawl all of the news article's content from these links and store as data\n",
    "# 4. Format all of the news article content to make them vectorizable by a word2vec model\n",
    "# 5. Load the pre-trained Google-News Word2Vec model & turn all news article content into\n",
    "#       an average of all of the vector of its words\n",
    "# 6. Create a similarity matrix that stores the similarity between every pair of news articles\n",
    "# Everything above just need to be packged into a function + make a few changes to make it\n",
    "# ready for integration into a web app.\n",
    "# 7. Test case - Simulating some random user myself\n",
    "\n",
    "##############################     1     ##############################\n",
    "# Set the sources that we are going to crawl from\n",
    "# Since this is a draft we won't worry about #sourcequality yet.\n",
    "# The sources are decided from this list: https://www.top10.com/news-websites\n",
    "# There isn't New York Times & NPR in Newsapi, so it won't be included either for now\n",
    "# Google News is a search engine that shows news from other news websites, which makes crawling\n",
    "# too hard, so won't be included either\n",
    "sources = 'cnn,reuters,the-wall-street-journal,bbc-news,fox-news,nbc-news,the-washington-post'\n",
    "# Note: To see all of the sources offered by Newsapi, visit: https://newsapi.org/v2/sources?apiKey=e938341216df4163be5f15cb92d413e6\n",
    "\n",
    "# Use a dictionary of dictionaries to set the tags & attributes for which news\n",
    "# content is stored in each of the above news sources, will be used later in crawling\n",
    "contentLocation = {\n",
    "    'cnn': {\n",
    "        'tag': 'p',\n",
    "        'class': 'paragraph inline-placeholder'\n",
    "    },\n",
    "    'reuters': {\n",
    "        'tag': 'p',\n",
    "        'class': 'text__text__1FZLe text__dark-grey__3Ml43 text__regular__2N1Xr text__large__nEccO body__full_width__ekUdw body__large_body__FV5_X article-body__element__2p5pI'\n",
    "    },\n",
    "    'the-wall-street-journal': {\n",
    "        'tag': 'p',\n",
    "        'class': 'css-xbvutc-Paragraph e3t0jlg0'\n",
    "    },\n",
    "    'bbc-news': {\n",
    "        'tag': 'p',\n",
    "        'class': 'ssrcss-1q0x1qg-Paragraph eq5iqo00'\n",
    "    },\n",
    "    'fox-news': {\n",
    "        'tag': 'p' # Fox news' article content has no attribute but just a tag <p>\n",
    "    },\n",
    "    'nbc-news': {\n",
    "        'tag': 'p',\n",
    "        'class': '' # Content is stored under the class attribute with no value on NBC news\n",
    "    },\n",
    "    'the-washington-post': {\n",
    "        'tag': 'p',\n",
    "        'class': 'wpds-c-cYdRxM wpds-c-cYdRxM-iPJLV-css font-copy'\n",
    "    }\n",
    "}\n",
    "\n",
    "##############################     2     ##############################\n",
    "# use headers to hide our API key\n",
    "headers = {'Authorization': 'e938341216df4163be5f15cb92d413e6'}\n",
    "\n",
    "# Set the API endpoint to crawl data from\n",
    "everything = \"https://newsapi.org/v2/everything?\"\n",
    "top_headlines = \"https://newsapi.org/v2/top-headlines?\"\n",
    "\n",
    "# Define keyword for how sources will be sorted\n",
    "sorby = \"popularity\"\n",
    "\n",
    "# Store keywords into a dictionary for use in crawling\n",
    "params = {'apiKey': apiKey,\n",
    "          'sources': sources,\n",
    "          'sortBy': sorby,\n",
    "          'language': 'en',\n",
    "          'page': 1}\n",
    "\n",
    "# Set html requests and get a response object, this is first run to establish\n",
    "# the total number of articles that we need to crawl\n",
    "response = requests.get(url = everything, headers = headers, params = params)\n",
    "\n",
    "# Turn response into a json object, and get the 'totalResults' field\n",
    "output = response.json()\n",
    "totalResults = output['totalResults']\n",
    "# Since each crawl gives a maximum of 100 articles, we would need to crawl\n",
    "# floor division by 100 + 1 times in total\n",
    "totalCrawlsNeeded = totalResults//100 + 1\n",
    "\n",
    "# However, due to using an upaid plan, I am actually only allowed 100 requests\n",
    "# per day. Thus this will be instead set to 100\n",
    "totalCrawlsNeeded = 100\n",
    "\n",
    "# Crawl all of the article urls and store them into our dataframe\n",
    "for i in range(1, totalCrawlsNeeded+1):\n",
    "    # Set the page number crawled in this run\n",
    "    params['page'] = i\n",
    "\n",
    "    # Send html requests and get response\n",
    "    response = requests.get(url = everything, headers = headers, params = params)\n",
    "\n",
    "    # Turn response into a json object\n",
    "    output = response.json()\n",
    "\n",
    "    ## Turn the json result into a pandas.dataframe object\n",
    "\n",
    "    # Our unpaid version limits the number of urls we can get, thus we need to wrap\n",
    "    # the next line in a try, except block\n",
    "    try:\n",
    "        # Variable to hold the list of article information\n",
    "        articles = output['articles']\n",
    "    except KeyError:\n",
    "        break\n",
    "\n",
    "    # Create dataframe from the list of dictionaries, if not present, else just concatenate\n",
    "    try:\n",
    "        df\n",
    "        temp = pd.DataFrame(articles)\n",
    "        df = pd.concat([df, temp], ignore_index = True)\n",
    "    except NameError:\n",
    "        df = pd.DataFrame(articles)        \n",
    "\n",
    "    # Sleep for 2 seconds to avoid overloading the API\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################     3     ##############################\n",
    "# Note: This code cell takes about 33 minuts to run, since it sends \n",
    "# about 500 html requests, takes about 2 seconds for every request, and\n",
    "#  waits for 2 seconds after every request -> 500 * 4 / 60 â‰ˆ 33 minutes\n",
    "# This is problematic and will be decreased in future iterations of the\n",
    "# product.\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "\n",
    "def get_news_content(url, tag, class_=None):\n",
    "    '''\n",
    "    Crawls a new's content, given the url, tag, and class.\n",
    "\n",
    "    Returns True, news_content if crawling was successful, else\n",
    "    returns False, [url, error_type, error_info] if error was encountered.\n",
    "    '''\n",
    "    # Since our code might produce errors in the process for various\n",
    "    # reasons, calling it in a try-except block will make the code run better\n",
    "    try:\n",
    "        # Send requests to the url & obtain response object\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Use BeautifulSoup to parse the html response & finding data\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the news content using the given tag & attribute\n",
    "        if class_:\n",
    "            content = soup.find_all(tag, class_ = class_)\n",
    "        else:\n",
    "            content = soup.find_all(tag)\n",
    "\n",
    "        # Content is a list of all the html elements found, we need to \n",
    "        # further concatenate them together into a string and strip it\n",
    "        news_content = ''\n",
    "        for tag_found in content:\n",
    "            news_content += tag_found.text + ' '\n",
    "\n",
    "        # Return True & content\n",
    "        return True, news_content\n",
    "\n",
    "    # this describes what to do if an exception is thrown \n",
    "    except Exception:\n",
    "        \n",
    "        # get the exception information\n",
    "        error_type, error_obj, error_info = sys.exc_info()\n",
    "        \n",
    "        # Return False & error info\n",
    "        return False, [url, error_type, error_obj, error_info]\n",
    "\n",
    "# Create a list to hold all of the failure info\n",
    "failure_info = []\n",
    "\n",
    "# Crawl news article data from all of the urls in the df\n",
    "for index, row in df.iterrows():\n",
    "    # Get the url\n",
    "    url = row['url']\n",
    "\n",
    "    # Get the id of the news website, then obtain tag & class info\n",
    "    # using our predefined dictionary\n",
    "    id = row['source']['id']\n",
    "    tag = contentLocation[id]['tag']\n",
    "    # Fox news is the only news website where its article content\n",
    "    # doesn't have any html attribute but just a tag <p>\n",
    "    if contentLocation[id] != 'fox-news':\n",
    "        class_ = contentLocation[id]['class']\n",
    "\n",
    "    # Crawl news content given the inputs\n",
    "    if class_:\n",
    "        successful, content = get_news_content(url, tag, class_)\n",
    "    else:\n",
    "        successful, content = get_news_content(url, tag)\n",
    "\n",
    "    # If successful, then replace the 'content' section of our df with the content\n",
    "    # Which is the last column, thus can be accessed by df.iloc[index, -1]\n",
    "    if successful:\n",
    "        df.iloc[index, -1] = content\n",
    "    # If failed then we gather the failure's info\n",
    "    else:\n",
    "        failure_info.append(content)\n",
    "\n",
    "    # Sleep for 2 seconds to avoid overloading\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(failure_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\#cs110-PythonProgramming:\n",
    "\n",
    "Explicitly shows that the PythonProgram passes verification tests.\n",
    "\n",
    "The placeholder list failure_info is completely empty shows that the above function of get_news_content() worked perfectly without any errors. There are certain runs in which the news_content variable returned from the function get_news_content() is empty, but that was due to the attribute for which data is stored is incorrect (i.e., BeautifulSoup.find_all() returned nothing because it was not able to find any data stored under such attribute), and not because the function is not running properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################     4     ##############################\n",
    "# First we need to do some formatting. Right now the 'content' column\n",
    "# of our df is really messy. Let's define a function to do that\n",
    "def format_string(text):\n",
    "    '''\n",
    "    Formats a string of text to make it more standardized. Includes\n",
    "    the following operations in the exact order:\n",
    "    1. Removes contractions, e.g. I'll -> I will\n",
    "    2. Removes punctuations, e.g. That is it. -> That is it\n",
    "    3. Removes numbers, e.g. 300 turtles -> turtles\n",
    "    4. Removes extra space, e.g. you  are right -> you are right\n",
    "    5. Makes words lowercase, e.g. Terminal -> terminal\n",
    "    6. Removes stop words (i.e. words that don't add value to our analysis),\n",
    "        e.g. the library -> library\n",
    "    '''\n",
    "    # 1. Removes contractions\n",
    "    from contractions import fix\n",
    "    text = fix(text)\n",
    "\n",
    "    # 2. Removes punctuation\n",
    "    from string import punctuation\n",
    "    translator = str.maketrans(punctuation, ' '*len(punctuation)) # map punctuation to space\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # 3. Removes numbers\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    # 4. Removes extra space\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # 5. Makes lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 6. Removes stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    text = \" \".join([word for word in text.split() if word not in (stop)])\n",
    "\n",
    "    # Return the result\n",
    "    return text\n",
    "\n",
    "# Next we are going to format all of the news article content\n",
    "# Define a list to hold all of the formatted strings\n",
    "formatted_contents = []\n",
    "for index, row in df.iterrows():\n",
    "    # Format the text\n",
    "    formatted_string = format_string(row['content'])\n",
    "\n",
    "    # Add the formatted text to list\n",
    "    formatted_contents.append(formatted_string)\n",
    "\n",
    "# Insert this list as a new column into our df\n",
    "df['Formatted content'] = formatted_contents\n",
    "\n",
    "# If formatted content is empty, then something went wrong in the previous\n",
    "# process (likely with web crawling). To avoid it interrupting subsequent\n",
    "# code, we are going to delete the row from the df\n",
    "delete_indexes = []\n",
    "for index, row in df.iterrows():\n",
    "    if not row['Formatted content']:\n",
    "        delete_indexes.append(index)\n",
    "\n",
    "df = df.drop(index = delete_indexes)\n",
    "# Reset indexes\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################     5     ##############################\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-tained google-news model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Define a function, that takes the model & a string of text as input\n",
    "# and outputs the averaged vector over the text.\n",
    "def text_to_average_vector(model, text):\n",
    "    '''\n",
    "    Using the gensim model, converts each word in the text string into\n",
    "    a vector, averages over all these vectors, and returns the average.\n",
    "    '''\n",
    "    # Split the text into a list of words\n",
    "    words = text.split()\n",
    "\n",
    "    # Create an empty numpy array, with ncol = number of words, and\n",
    "    # nrow = output dimensions of the model\n",
    "    all_vectors = np.zeros((model.vector_size, len(words)))\n",
    "\n",
    "    # If our model is large, then it would be reasonable for us to assume that \n",
    "    # any word that cannot be found in the model is not a word, e.g. Ã¢ or Â©\n",
    "    # If such words are encountered, it also means that we need to delete a\n",
    "    # column from our vector of all the words, to avoid disrupting the subsequent\n",
    "    # average, thus define a list to hold the column indexes that we are going to\n",
    "    # delete later:\n",
    "    col_to_delete = []\n",
    "\n",
    "    # Looping over all words, turn them into vectors and insert into np array\n",
    "    for i in range(len(words)):\n",
    "        try:\n",
    "            vector = model.get_vector(words[i])\n",
    "        # If the word cannot be found in the model, we will need to delete this\n",
    "        # column from our array of vectors\n",
    "        except KeyError:\n",
    "            col_to_delete.append(i)\n",
    "            continue\n",
    "\n",
    "        # Insert this vector at the right position\n",
    "        # a[:, 0] means select all rows from column 0\n",
    "        all_vectors[:, i] = vector\n",
    "\n",
    "    # Delete the columns in which the word cannot be found in the model\n",
    "    final_vec = np.delete(all_vectors, col_to_delete, axis = 1)\n",
    "\n",
    "    # Average over the columns, and return the averaged vector\n",
    "    averaged_vec = np.mean(final_vec, axis = 1)\n",
    "    return averaged_vec\n",
    "\n",
    "# For all of our news articles, obtain an averaged vector of its news content,\n",
    "# and add as a new column to our df\n",
    "df_vectors = []\n",
    "for index, row in df.iterrows():\n",
    "    news_vector = text_to_average_vector(model, row['Formatted content'])\n",
    "    df_vectors.append(news_vector)\n",
    "\n",
    "df['Vector'] = df_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################     6     ##############################\n",
    "# I've tried the n_similarity function given by the model, but it did\n",
    "# not work so well. I think it is because of the enormous amount of words\n",
    "# that made the model think every pair of articles is similar. The minimum\n",
    "# of all of them is 0.97 (which does not make enough sense), and it takes\n",
    "# a long time to run as well. Thus instead I created an average vector on my\n",
    "# own, and computed their cosine similarity. It is still not very good, but\n",
    "# it's better than all news articles being similar (good enough for a 1st draft).\n",
    "\n",
    "# This functions scales with Î¸(n^2), which makes it take a lot of time when\n",
    "# there are a lot of news articles. It will be improved in the next iteration.\n",
    "def make_similarity_matrix(iterable):\n",
    "    '''\n",
    "    Creates a similarity matrix given an iterable of vectors that each\n",
    "    represents the semantic position of a news article.\n",
    "    Requires an iterable of vectors as input.\n",
    "    '''\n",
    "    # Create an empty matrix\n",
    "    similarity_matrix = np.zeros((len(iterable), len(iterable)))\n",
    "    # Import cosince similarity from sklearn to compute similarity scores\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    # Looping over all the entries in the iterable\n",
    "    for i in range(len(iterable)):\n",
    "        # Compare row's vector against every other row's vector\n",
    "        for j in range(len(iterable)):\n",
    "            # Reshape the vectors for use in sklearn's function\n",
    "            vec1 = iterable[i].reshape(1, -1)\n",
    "            vec2 = iterable[j].reshape(1, -1)\n",
    "\n",
    "            # Compute score using cosine similarity\n",
    "            score = cosine_similarity(vec1, vec2)\n",
    "\n",
    "            # Add the score to our matrix\n",
    "            similarity_matrix[i, j] = score\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "# Make a similarity matrix for all the news articles that we've recorded\n",
    "similarity_matrix = make_similarity_matrix(df['Vector'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\#cs110 - Complexity Analysis: \n",
    "\n",
    "For this function \"make_similarity_matrix(iterable)\", we can analyze its scaling behavior and see that it scales with $\\Theta(n^2)$, where $n$ is the length of this \"iterable\" in its input. This is because the function utilizes two for loops that first goes over all of the items once in the iterable, then for each item it goes over all of the items in the iterable a second time, to compute a cosine similarity score between every pair of items in the iterable. Computing the cosine similarity score might also be costly if the vectors in the iterable are of a large size, but that would only be multiplying a constant on top of the $n^2$ complexity (given that $n$ is large and it is since we are analyzing the function's asymptotic behavior, i.e., when $n$ is large), which justifies our analysis of its scaling behavior being $\\Theta(n^2)$. The Big-$\\Theta$ notation is the better one to be used here, because there is no conditional statement in the for loops that gives us a better case or a worst case, which makes describing its scaling behavior using the average case $\\Theta$ the best choice. This complexity of this function could be improved if conditional statements were included in the two for loops to avoid repeatedly calculating the same score again, since the cosine similarity score between two vectors does not change when their order is reversed, additionally the cosine similarity score between two identical vectors is just 1. This is not done in this iteration of the product since this is just the first draft, but in future iterations in order to minimize the time taken for this operation, this improvement will likely be implemented, which should improve the complexity of this function to $\\Theta(n^2/2-n)$, with the division by 2 due to removing all the duplicate calculations, and minus $n$ due to simply equating all the calculation between identical vectors to be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.74486999, 0.57070244, ..., 0.8250236 , 0.79389395,\n",
       "        0.68474844],\n",
       "       [0.74486999, 1.        , 0.59013527, ..., 0.78268445, 0.72266448,\n",
       "        0.57170482],\n",
       "       [0.57070244, 0.59013527, 1.        , ..., 0.55343187, 0.57222319,\n",
       "        0.5589997 ],\n",
       "       ...,\n",
       "       [0.8250236 , 0.78268445, 0.55343187, ..., 1.        , 0.82387438,\n",
       "        0.65693511],\n",
       "       [0.79389395, 0.72266448, 0.57222319, ..., 0.82387438, 1.        ,\n",
       "        0.70694738],\n",
       "       [0.68474844, 0.57170482, 0.5589997 , ..., 0.65693511, 0.70694738,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In a deployment environment, this matrix will be stored in the server,\n",
    "# and updated every once in a while\n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'China has recorded its highest number of daily Covid cases since the pandemic began, despite stringent measures designed to eliminate the virus. Several major cities including the capital Beijing and southern trade hub Guangzhou are experiencing outbreaks. Wednesday saw 31,527 cases recorded compared with an April peak of 28,000. The numbers are still tiny for a country of 1.4 billion people and officially just over 5,200 have died since the pandemic began. That equates to three Covid deaths in every million in China, compared with 3,000 per million in the US and 2,400 per million in the UK, although direct comparisons between countries are difficult. While China\\'s zero-Covid policy has clearly saved lives, it has also dealt a punishing blow to the economy and ordinary people\\'s lives. The country slightly relaxed some of those restrictions a few weeks ago.  It cut quarantine for close contacts from seven days in a state facility to five days and three days at home, and stopped recording secondary contacts which allowed many more people to avoid having to quarantine. Officials have also sought to avoid enforcing blanket lockdowns of the kind endured by the largest city, Shanghai, earlier this year. But faced with a renewed surge in cases in Beijing, as well as the first deaths from the virus in months, officials have already implemented some restrictions in several districts, with shops, schools and restaurants closed. The central city of Zhengzhou is also to enforce an effective lockdown for 6 million residents from Friday, officials announced. It follows violent protests at a vast industrial complex belonging to iPhone manufacturer Foxconn. The firm has apologised for a \"technical error\" in its payment systems. This video can not be played WATCH: Chinese protesters clash with riot police at giant iPhone factory Other stories of suffering and desperation have been shared online where they have fuelled public resentment. Last week, reports that a baby in Zhengzhou died because her medical care was delayed by Covid restrictions prompted a huge outcry. Among some of the most severe responses to Covid this year: The word came suddenly last night that our housing compound would be locked down with all residents confined to their homes. This is not a surprise in China anymore. At any point, a single infection or being linked to an infection can mean not being allowed out. Beijing is in the middle of a major Covid outbreak but, even before then, visiting a shopping mall or a building where an infected person had been meant going into home quarantine. Right now, in the capital\\'s vast Chaoyang district most businesses are closed. In thousands of tower blocks, all residents have been ordered to remain indoors for the next few days initially. Then consider that this is being replicated in cities right across the country. In the regions of Xinjiang and Tibet, the lockdowns have gone on for months. At the beginning of 2023, this country will be heading into its fourth year of this crisis and, whether it is true or not, zero-Covid has a never-ending feeling to it. People are completely worn out by the pandemic and the government\\'s economy-destroying response to it. That officials have not explained where the off ramp is, has only added to the uncertainty. Scientists here can also see that China\\'s vaccination rates are way too low, especially amongst vulnerable groups. What is more, not enough resources have been diverted into expanding medical facilities to cope with a massive influx of patients following any opening-up. In the short term though, my compound has now told me that, after several rounds of mass testing, we are allowed to leave. For most residents this will not make much difference though because hardly anything outside is open, including their workplaces. China is the last major economy still pursuing a Covid eradication process with mass testing and lockdown rules, and virus cases are being recorded in 31 provinces. Part of the reason is that vaccination levels are lower than in other developed nations, and only half of people aged over 80 have their primary vaccinations. China has refused to import vaccines despite evidence that its homemade jabs have not proved as effective. President Xi Jinping argues that strict curbs are needed to protect the country\\'s large elderly population. Zero-Covid has come to define his rule and the authoritarian bureaucracy at his disposal like almost no other policy. It projects a veneer of control and stability in the run-up to March when China\\'s equivalent of a parliament will convene to choose Mr Xi as president for a third time. \"Lockdowns prevent Covid outbreaks from spreading,\" William Hurst, professor of Chinese development at Cambridge University, told BBC News recently. \"But they also exert incredibly strict social control.\" Ã‚Â© 2022 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################     7     ##############################\n",
    "# (Intended) Upon login, 10 random articles is shown to the user to\n",
    "# learn their preference.\n",
    "# Choose 10 random articles, show them to the user, obtain their \n",
    "# satisfaction score on these articles on a scale of 1 to 10, with\n",
    "# 10 the most satisfied and 1 being the least satisfied, then compute\n",
    "# the user's preference vector by averaging over these vectors using\n",
    "# their rating as the weights.\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set numpy random seed to ensure replicability\n",
    "np.random.seed(125)\n",
    "ten_random_articles = np.random.choice(df.index, size=10, replace=False)\n",
    "\n",
    "# Holder variable to record vector and score\n",
    "user_scores = []\n",
    "\n",
    "# Looping over the 10 articles\n",
    "for index in ten_random_articles:\n",
    "    # Show the article to the user and inquire a score from the user\n",
    "    score = int(input(\"Please give a satisfaction score in the range of 1 to 10 on the recommended article: \\n\\n\" + df['content'][index]))\n",
    "\n",
    "    # Turn it into a dictionary and add to holder list\n",
    "    temp = {'vector': df['Vector'][index],\n",
    "            'score' : score}\n",
    "    user_scores.append(temp)\n",
    "\n",
    "# Obtain an average of the user's vector weighted by score, to generate the initial\n",
    "# learned preference of the user\n",
    "sum_vector = np.zeros((model.vector_size, ))\n",
    "for i in range(len(user_scores)):\n",
    "    weighted_score = user_scores[i]['score'] / 10\n",
    "    \n",
    "    sum_vector += weighted_score * user_scores[i]['vector']\n",
    "\n",
    "# Average over all vectors to get the initial preference\n",
    "user_preference = sum_vector / len(user_scores)\n",
    "user_preference = user_preference.reshape(1, -1)\n",
    "\n",
    "# Find the article that is most similar to the learned preference of the user,\n",
    "# Excluding the articles that was used for training\n",
    "df_user = df.drop(index=ten_random_articles)\n",
    "similarities = []\n",
    "for index, row in df_user.iterrows():\n",
    "    vector_article = row['Vector'].reshape(1, -1)\n",
    "    sim = cosine_similarity(user_preference, vector_article)\n",
    "    similarities.append(sim)\n",
    "\n",
    "index = similarities.index(max(similarities))\n",
    "\n",
    "df['content'][index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analysis (#cs156 - Overfitting)\n",
    "\n",
    "The above results are reasonable and expected, since I gave two ratings of 9 to two Ukrainian war news articles, and two 8s and one 7 to three other political articles (if you want to check out the articles out themselves, you can run all of the above code cells, it should output the same data), and the most similar piece of text that the model recommends is also about the Ukrainian war. It makes perfect sense for the recommender to be recommending news articles about the Ukrainian war, after it sees that I am interested in the Ukranian war and politics. This is defintely not the ideal version of the recommender that we want, since we want it to be able to extrapolate our preference and recommend thing that we do not know we are interested in.\n",
    "\n",
    "I have thought about a few interesting applications that might strength this current Word2Vec model, and I will be testing one of them out in the next iteration of the product. They are:\n",
    "\n",
    "1. Instead of recommending articles that has the closest similarity to the learned user's preference, we always attempt to recommend articles that are a given distance away from the user's preference. In other words, if we were to think of the endpoint of the vector of user's preference as a point in a high-dimensional word-embedding space, we always attempt to recommend articles that is on the surface of a hyper-sphere that has its center as the preference of the user. With every news article that we recommend to the user, instead of asking for the user to score their satisfaction, we ask the user whether this is too convergent to their ideas (by which we move the center of the hyper-sphere away from this point), or too divergent to their ideas (by which we move the center of the hyper-sphere closer to this point); and whether they would like more convergent new (by which we decrease the radius of the hyper-sphere), or they would like more divergent news (by which we then increase the radius of the hyper-sphere).\n",
    "\n",
    "2. The limitation of the above method is that it allows users to create their own \"media bubble\". Stubborn people only gets more stubborn. A potential improvement to the model is if we do not allow the user to control that. In addition, we include some randomness into the model prediction, by adding a random vector of a set magnitude on top of the learned user's preference every time we are generating a news recommendation, that is to say, instead of recommending news articles that are the most similar to the preference of the user, we recommend news articles that are always a set semantic distance away from the preference of the user (in a random direction). Obviously, the limitation of this method is that the setting of the magnitude of the random vector becomes very influential in the recommendation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "# Prototype of how user preference learning will be like (next step)\n",
    "# Here's how I imagined the user preference learning process to be:\n",
    "# 1. User logins onto their account and their preference will be learned\n",
    "#       by supplying 10 articles for them to rate on a scale of 1 to 10,\n",
    "#       with respect to how satisfied they are with the article with 10\n",
    "#       being the most satisfied and 1 being not satisfied at all.\n",
    "# 2. After the rating of each article, we learn a little bit more about\n",
    "#       the user. We aim to give user the most semantically different\n",
    "#       article to rate (by maximizing the difference in similarity\n",
    "#       score compared to their previous ratings), to get more info of\n",
    "#       the user.\n",
    "# 3. After the user rates all 10 articles, we will get an average of these\n",
    "#       vectors, weighted by the ratings they gave to each of them. This\n",
    "#       will be the initial learned preference of the user.\n",
    "# 4. Whenever the user clicks on an article recommended to them (which pops\n",
    "#       up in a separate window), the website will show a window which asks\n",
    "#       them to rate the article that they just read on a scale of 1 to 10.\n",
    "#       We use this to learn a little bit further about the user.\n",
    "\n",
    "\n",
    "# Taking myself as the test case, I am going to simulate being a user here\n",
    "# Create a holder list that will hold all of the vectors that the user\n",
    "# has rated, the index of the article, and their rating of the articles\n",
    "zichen = []\n",
    "# Set a seed to ensure replicability\n",
    "rd.seed(12454)\n",
    "# Determine a random first article to show\n",
    "first_article_index = rd.choice(df.index)\n",
    "\n",
    "# Hidden here to avoid excessive amount of output, but basically it was\n",
    "# about shooting crimes in Virginia which I honestly don't care for,\n",
    "# so I am going to give it a score of 1\n",
    "#df['content'][first_article_index]\n",
    "rating1 = {'vector': df['Vector'][first_article_index],\n",
    "           'index' : first_article_index,\n",
    "           'score' : 1}\n",
    "zichen.append(rating1)\n",
    "\n",
    "# Next we are going to find the most different article from the first one\n",
    "# to give to the user for rating, so lets find the least similar article\n",
    "# from our similarity matrix compared to the first article shown\n",
    "first_sim = similarity_matrix[first_article_index]\n",
    "second_article_indexes = np.where(first_sim == first_sim.min())\n",
    "\n",
    "# We found two, 63 and 147, so let's just randomly choose one\n",
    "if rd.random() < 0.5:\n",
    "    second_article_index = 63\n",
    "else:\n",
    "    second_article_index = 147\n",
    "\n",
    "# The random run gave 63, now show the second article\n",
    "# Hidden to avoid excessive output\n",
    "df['content'][second_article_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de264d6f37cb5440ad063971cae3b1d83d14f2649fd07ea20b1c93af5fbd4acc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
